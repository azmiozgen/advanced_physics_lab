\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\@writefile{toc}{\contentsline {section}{Acknowledgements}{iv}{section*.1}}
\@writefile{toc}{\contentsline {section}{Abbreviations}{v}{section*.2}}
\citation{ref1}
\@writefile{toc}{\contentsline {section}{1. Introduction}{1}{section*.3}}
\newlabel{introduction}{{}{1}{1. Introduction}{section*.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces 100 Handwritten Digits}}{1}{figure.1}}
\@writefile{toc}{\contentsline {subsection}{1.1. Perceptrons}{1}{subsection*.4}}
\newlabel{perceptrons}{{}{1}{1.1. Perceptrons}{subsection*.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Perceptron}}{2}{figure.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Multilayer Perceptron (MLP)}}{2}{figure.3}}
\@writefile{toc}{\contentsline {subsection}{1.2. Sigmoid neurons}{3}{subsection*.5}}
\newlabel{sigmoid-neurons}{{}{3}{1.2. Sigmoid neurons}{subsection*.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Effects of changing weights to the output}}{4}{figure.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Sigmoid function}}{5}{figure.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Step function}}{5}{figure.6}}
\@writefile{toc}{\contentsline {subsection}{1.3. The architecture of neural networks}{6}{subsection*.6}}
\newlabel{the-architecture-of-neural-networks}{{}{6}{1.3. The architecture of neural networks}{subsection*.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Simple architecture}}{7}{figure.7}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Layers}}{7}{figure.8}}
\@writefile{toc}{\contentsline {subsection}{1.4. A simple network to classify handwritten digits}{8}{subsection*.7}}
\newlabel{a-simple-network-to-classify-handwritten-digits}{{}{8}{1.4. A simple network to classify handwritten digits}{subsection*.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Seperate digits}}{8}{figure.9}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Three layered feedforward network}}{9}{figure.10}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Some parts of number 0}}{9}{figure.11}}
\@writefile{toc}{\contentsline {subsection}{1.5. Learning with gradient descent}{9}{subsection*.8}}
\newlabel{learning-with-gradient-descent}{{}{9}{1.5. Learning with gradient descent}{subsection*.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces MNIST data set samples}}{10}{figure.12}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Sample cost function with double variables}}{11}{figure.13}}
\citation{ref2}
\@writefile{toc}{\contentsline {subsection}{1.6. Implementing our network to classify digits}{14}{subsection*.9}}
\newlabel{implementing-our-network-to-classify-digits}{{}{14}{1.6. Implementing our network to classify digits}{subsection*.9}{}}
\@writefile{toc}{\contentsline {section}{2. How the backpropagation algorithm works}{15}{section*.10}}
\newlabel{how-the-backpropagation-algorithm-works}{{}{15}{2. How the backpropagation algorithm works}{section*.10}{}}
\@writefile{toc}{\contentsline {subsection}{2.1. A fast matrix-based approach to computing the output from a neural network}{16}{subsection*.11}}
\newlabel{warm-up-a-fast-matrix-based-approach-to-computing-the-output-from-a-neural-network}{{}{16}{2.1. A fast matrix-based approach to computing the output from a neural network}{subsection*.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Weight notation}}{16}{figure.14}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Bias and activation notation}}{16}{figure.15}}
\@writefile{toc}{\contentsline {subsection}{2.2. The Hadamard product, $ s \odot t $}{17}{subsection*.12}}
\newlabel{the-hadamard-product-s-t}{{}{17}{2.2. The Hadamard product, $ s \odot t $}{subsection*.12}{}}
\@writefile{toc}{\contentsline {subsection}{2.3. The four fundamental equations behind backpropagation}{17}{subsection*.13}}
\newlabel{the-four-fundamental-equations-behind-backpropagation}{{}{17}{2.3. The four fundamental equations behind backpropagation}{subsection*.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Four fundamental equations of backpropapagation}}{20}{figure.16}}
\@writefile{toc}{\contentsline {subsection}{2.4. The backpropagation algorithm}{20}{subsection*.14}}
\newlabel{the-backpropagation-algorithm}{{}{20}{2.4. The backpropagation algorithm}{subsection*.14}{}}
\@writefile{toc}{\contentsline {section}{3. Improving the way neural networks learn}{21}{section*.15}}
\newlabel{improving-the-way-neural-networks-learn}{{}{21}{3. Improving the way neural networks learn}{section*.15}{}}
\@writefile{toc}{\contentsline {subsection}{3.1. The cross-entropy cost function}{22}{subsection*.16}}
\newlabel{the-cross-entropy-cost-function}{{}{22}{3.1. The cross-entropy cost function}{subsection*.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Simple neuron}}{22}{figure.17}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Single neuron with weight=0.6, bias=0.9}}{24}{figure.18}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces Single neuron with weight=2.0, bias=2.0}}{25}{figure.19}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces Sigmoid function}}{26}{figure.20}}
\@writefile{toc}{\contentsline {subsection}{3.2. Introducing the cross-entropy cost function}{26}{subsection*.17}}
\newlabel{introducing-the-cross-entropy-cost-function}{{}{26}{3.2. Introducing the cross-entropy cost function}{subsection*.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces A neuron with multiple inputs}}{26}{figure.21}}
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces Single neuron with cross entropy cost with weight=0.6, bias=0.9}}{28}{figure.22}}
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces Single neuron with cross entropy cost with weight=2.0, bias=2.0}}{29}{figure.23}}
\@writefile{toc}{\contentsline {subsection}{3.3. Softmax}{29}{subsection*.18}}
\newlabel{softmax}{{}{29}{3.3. Softmax}{subsection*.18}{}}
\@writefile{toc}{\contentsline {subsection}{3.4. Overfitting}{31}{subsection*.19}}
\newlabel{overfitting}{{}{31}{3.4. Overfitting}{subsection*.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {24}{\ignorespaces Overfitting effect on the cost of training data}}{32}{figure.24}}
\@writefile{lof}{\contentsline {figure}{\numberline {25}{\ignorespaces Overfitting effect on the accuracy of test data}}{32}{figure.25}}
\@writefile{lof}{\contentsline {figure}{\numberline {26}{\ignorespaces Overfitting effect on the accuracy of training data}}{33}{figure.26}}
\@writefile{lof}{\contentsline {figure}{\numberline {27}{\ignorespaces Comparison of accuracies in training and test data}}{34}{figure.27}}
\@writefile{toc}{\contentsline {subsection}{3.5. Regularization}{35}{subsection*.20}}
\newlabel{regularization}{{}{35}{3.5. Regularization}{subsection*.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {28}{\ignorespaces Cost in training data for regularized case}}{37}{figure.28}}
\@writefile{lof}{\contentsline {figure}{\numberline {29}{\ignorespaces Accuracy in test data for regularized case}}{38}{figure.29}}
\@writefile{lof}{\contentsline {figure}{\numberline {30}{\ignorespaces Comparison of accuracies of training and test data}}{39}{figure.30}}
\@writefile{toc}{\contentsline {subsection}{3.5.1. Other techniques for regularization}{39}{subsection*.21}}
\newlabel{other-techniques-for-regularization}{{}{39}{3.5.1. Other techniques for regularization}{subsection*.21}{}}
\@writefile{toc}{\contentsline {subsubsection}{3.5.1.1. L1 regularization}{39}{subsubsection*.22}}
\newlabel{l1-regularization}{{}{39}{3.5.1.1. L1 regularization}{subsubsection*.22}{}}
\@writefile{toc}{\contentsline {subsubsection}{3.5.1.2. Dropout}{40}{subsubsection*.23}}
\newlabel{dropout}{{}{40}{3.5.1.2. Dropout}{subsubsection*.23}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {31}{\ignorespaces MLP}}{41}{figure.31}}
\@writefile{lof}{\contentsline {figure}{\numberline {32}{\ignorespaces Dropping neurons out}}{41}{figure.32}}
\@writefile{lof}{\contentsline {figure}{\numberline {33}{\ignorespaces Dropout effect on train accuracy}}{42}{figure.33}}
\@writefile{lof}{\contentsline {figure}{\numberline {34}{\ignorespaces Dropout effect on test accuracy}}{43}{figure.34}}
\@writefile{lof}{\contentsline {figure}{\numberline {35}{\ignorespaces Dropout effect on test accuracy}}{43}{figure.35}}
\@writefile{toc}{\contentsline {subsection}{3.7. How to choose a neural network's hyper-parameters?}{44}{subsection*.24}}
\newlabel{how-to-choose-a-neural-networks-hyper-parameters}{{}{44}{3.7. How to choose a neural network's hyper-parameters?}{subsection*.24}{}}
\@writefile{toc}{\contentsline {subsubsection}{3.7.1. Broad strategy}{44}{subsubsection*.25}}
\newlabel{broad-strategy}{{}{44}{3.7.1. Broad strategy}{subsubsection*.25}{}}
\@writefile{toc}{\contentsline {subsubsection}{3.7.2. Learning rate}{44}{subsubsection*.26}}
\newlabel{learning-rate}{{}{44}{3.7.2. Learning rate}{subsubsection*.26}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {36}{\ignorespaces Comparison of learning rates}}{45}{figure.36}}
\@writefile{toc}{\contentsline {subsubsection}{3.7.3. Use early stopping to determine the number of training epochs}{45}{subsubsection*.27}}
\newlabel{use-early-stopping-to-determine-the-number-of-training-epochs}{{}{45}{3.7.3. Use early stopping to determine the number of training epochs}{subsubsection*.27}{}}
\@writefile{toc}{\contentsline {subsubsection}{3.7.4. Learning rate schedule}{46}{subsubsection*.28}}
\newlabel{learning-rate-schedule}{{}{46}{3.7.4. Learning rate schedule}{subsubsection*.28}{}}
\@writefile{toc}{\contentsline {subsubsection}{3.7.5. The regularization parameter}{46}{subsubsection*.29}}
\newlabel{the-regularization-parameter}{{}{46}{3.7.5. The regularization parameter}{subsubsection*.29}{}}
\@writefile{toc}{\contentsline {subsubsection}{3.7.6. Mini-batch size}{46}{subsubsection*.30}}
\newlabel{mini-batch-size}{{}{46}{3.7.6. Mini-batch size}{subsubsection*.30}{}}
\@writefile{toc}{\contentsline {subsection}{3.8. Other techniques}{47}{subsection*.31}}
\newlabel{other-techniques}{{}{47}{3.8. Other techniques}{subsection*.31}{}}
\@writefile{toc}{\contentsline {subsubsection}{3.8.1. Hessian technique}{47}{subsubsection*.32}}
\newlabel{hessian-technique}{{}{47}{3.8.1. Hessian technique}{subsubsection*.32}{}}
\@writefile{toc}{\contentsline {subsubsection}{3.8.2. Momentum-based gradient descent}{48}{subsubsection*.33}}
\newlabel{momentum-based-gradient-descent}{{}{48}{3.8.2. Momentum-based gradient descent}{subsubsection*.33}{}}
\@writefile{toc}{\contentsline {subsection}{3.9. Other models of artificial neuron}{49}{subsection*.34}}
\newlabel{other-models-of-artificial-neuron}{{}{49}{3.9. Other models of artificial neuron}{subsection*.34}{}}
\citation{ref3}
\@writefile{lof}{\contentsline {figure}{\numberline {37}{\ignorespaces Tanh function}}{50}{figure.37}}
\bibcite{ref1}{1}
\bibcite{ref2}{2}
\bibcite{ref3}{3}
\@writefile{lof}{\contentsline {figure}{\numberline {38}{\ignorespaces Rectifying function}}{51}{figure.38}}
