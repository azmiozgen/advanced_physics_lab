\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\@writefile{toc}{\contentsline {section}{Abstract}{1}{section*.1}}
\newlabel{abstract}{{}{1}{Abstract}{section*.1}{}}
\@writefile{toc}{\contentsline {section}{1. Introduction}{1}{section*.2}}
\newlabel{introduction}{{}{1}{1. Introduction}{section*.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces 100 Handwritten Digits}}{1}{figure.1}}
\@writefile{toc}{\contentsline {subsection}{1.1. Perceptrons}{1}{section*.3}}
\newlabel{perceptrons}{{}{1}{1.1. Perceptrons}{section*.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Perceptron}}{2}{figure.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Multilayer Perceptron (MLP)}}{3}{figure.3}}
\@writefile{toc}{\contentsline {subsection}{1.2. Sigmoid neurons}{3}{section*.4}}
\newlabel{sigmoid-neurons}{{}{3}{1.2. Sigmoid neurons}{section*.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Effects of changing weights to the output}}{4}{figure.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Sigmoid function}}{5}{figure.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Step function}}{6}{figure.6}}
\@writefile{toc}{\contentsline {subsection}{1.3. The architecture of neural networks}{6}{section*.5}}
\newlabel{the-architecture-of-neural-networks}{{}{6}{1.3. The architecture of neural networks}{section*.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Simple architecture}}{7}{figure.7}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Layers}}{7}{figure.8}}
\@writefile{toc}{\contentsline {subsection}{1.4. A simple network to classify handwritten digits}{8}{section*.6}}
\newlabel{a-simple-network-to-classify-handwritten-digits}{{}{8}{1.4. A simple network to classify handwritten digits}{section*.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Seperate digits}}{8}{figure.9}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Three layered feedforward network}}{9}{figure.10}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Some parts of number 0}}{10}{figure.11}}
\@writefile{toc}{\contentsline {subsection}{1.5. Learning with gradient descent}{10}{section*.7}}
\newlabel{learning-with-gradient-descent}{{}{10}{1.5. Learning with gradient descent}{section*.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces MNIST data set samples}}{10}{figure.12}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Sample cost function with double variables}}{12}{figure.13}}
\@writefile{toc}{\contentsline {subsection}{1.6. Implementing our network to classify digits}{14}{section*.8}}
\newlabel{implementing-our-network-to-classify-digits}{{}{14}{1.6. Implementing our network to classify digits}{section*.8}{}}
\@writefile{toc}{\contentsline {section}{2. How the backpropagation algorithm works}{15}{section*.9}}
\newlabel{how-the-backpropagation-algorithm-works}{{}{15}{2. How the backpropagation algorithm works}{section*.9}{}}
\@writefile{toc}{\contentsline {subsection}{2.1. A fast matrix-based approach to computing the output from a neural network}{16}{section*.10}}
\newlabel{warm-up-a-fast-matrix-based-approach-to-computing-the-output-from-a-neural-network}{{}{16}{2.1. A fast matrix-based approach to computing the output from a neural network}{section*.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Weight notation}}{16}{figure.14}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Bias and activation notation}}{16}{figure.15}}
\@writefile{toc}{\contentsline {subsection}{2.2. The Hadamard product, $ s \odot t $}{17}{section*.11}}
\newlabel{the-hadamard-product-s-t}{{}{17}{2.2. The Hadamard product, $ s \odot t $}{section*.11}{}}
\@writefile{toc}{\contentsline {subsection}{2.3. The four fundamental equations behind backpropagation}{17}{section*.12}}
\newlabel{the-four-fundamental-equations-behind-backpropagation}{{}{17}{2.3. The four fundamental equations behind backpropagation}{section*.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Four fundamental equations of backpropapagation}}{19}{figure.16}}
\@writefile{toc}{\contentsline {subsection}{2.4. The backpropagation algorithm}{20}{section*.13}}
\newlabel{the-backpropagation-algorithm}{{}{20}{2.4. The backpropagation algorithm}{section*.13}{}}
\@writefile{toc}{\contentsline {section}{3. Improving the way neural networks learn}{21}{section*.14}}
\newlabel{improving-the-way-neural-networks-learn}{{}{21}{3. Improving the way neural networks learn}{section*.14}{}}
\@writefile{toc}{\contentsline {subsection}{3.1. The cross-entropy cost function}{21}{section*.15}}
\newlabel{the-cross-entropy-cost-function}{{}{21}{3.1. The cross-entropy cost function}{section*.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Simple neuron}}{21}{figure.17}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Single neuron with weight=0.6, bias=0.9}}{23}{figure.18}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces Single neuron with weight=2.0, bias=2.0}}{24}{figure.19}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces Sigmoid function}}{25}{figure.20}}
\@writefile{toc}{\contentsline {subsection}{3.2. Introducing the cross-entropy cost function}{25}{section*.16}}
\newlabel{introducing-the-cross-entropy-cost-function}{{}{25}{3.2. Introducing the cross-entropy cost function}{section*.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces A neuron with multiple inputs}}{25}{figure.21}}
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces Single neuron with cross entropy cost with weight=0.6, bias=0.9}}{27}{figure.22}}
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces Single neuron with cross entropy cost with weight=2.0, bias=2.0}}{28}{figure.23}}
\@writefile{toc}{\contentsline {subsection}{3.3. Softmax}{28}{section*.17}}
\newlabel{softmax}{{}{28}{3.3. Softmax}{section*.17}{}}
\@writefile{toc}{\contentsline {subsection}{3.4. Overfitting}{30}{section*.18}}
\newlabel{overfitting}{{}{30}{3.4. Overfitting}{section*.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {24}{\ignorespaces Overfitting effect on the cost of training data}}{31}{figure.24}}
\@writefile{lof}{\contentsline {figure}{\numberline {25}{\ignorespaces Overfitting effect on the accuracy of test data}}{31}{figure.25}}
\@writefile{lof}{\contentsline {figure}{\numberline {26}{\ignorespaces Overfitting effect on the accuracy of training data}}{32}{figure.26}}
\@writefile{lof}{\contentsline {figure}{\numberline {27}{\ignorespaces Comparison of accuracies in training and test data}}{33}{figure.27}}
\@writefile{toc}{\contentsline {subsection}{3.5. Regularization}{34}{section*.19}}
\newlabel{regularization}{{}{34}{3.5. Regularization}{section*.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {28}{\ignorespaces Cost in training data for regularized case}}{36}{figure.28}}
\@writefile{lof}{\contentsline {figure}{\numberline {29}{\ignorespaces Accuracy in test data for regularized case}}{36}{figure.29}}
\@writefile{lof}{\contentsline {figure}{\numberline {30}{\ignorespaces Comparison of accuracies of training and test data}}{37}{figure.30}}
\@writefile{toc}{\contentsline {subsection}{3.5.1. Other techniques for regularization}{38}{section*.20}}
\newlabel{other-techniques-for-regularization}{{}{38}{3.5.1. Other techniques for regularization}{section*.20}{}}
\@writefile{toc}{\contentsline {subsubsection}{3.5.1.1. L1 regularization}{38}{section*.21}}
\newlabel{l1-regularization}{{}{38}{3.5.1.1. L1 regularization}{section*.21}{}}
\@writefile{toc}{\contentsline {subsubsection}{3.5.1.2. Dropout}{39}{section*.22}}
\newlabel{dropout}{{}{39}{3.5.1.2. Dropout}{section*.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {31}{\ignorespaces MLP}}{40}{figure.31}}
\@writefile{lof}{\contentsline {figure}{\numberline {32}{\ignorespaces Dropping neurons out}}{41}{figure.32}}
\@writefile{lof}{\contentsline {figure}{\numberline {33}{\ignorespaces Dropout effect on train accuracy}}{41}{figure.33}}
\@writefile{lof}{\contentsline {figure}{\numberline {34}{\ignorespaces Dropout effect on test accuracy}}{42}{figure.34}}
\@writefile{lof}{\contentsline {figure}{\numberline {35}{\ignorespaces Dropout effect on test accuracy}}{42}{figure.35}}
\@writefile{toc}{\contentsline {subsubsection}{3.6. Artificially expanding the training data}{43}{section*.23}}
\newlabel{artificially-expanding-the-training-data}{{}{43}{3.6. Artificially expanding the training data}{section*.23}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {36}{\ignorespaces Normal number five}}{43}{figure.36}}
\@writefile{toc}{\contentsline {subsection}{3.7. How to choose a neural network's hyper-parameters?}{43}{section*.24}}
\newlabel{how-to-choose-a-neural-networks-hyper-parameters}{{}{43}{3.7. How to choose a neural network's hyper-parameters?}{section*.24}{}}
\@writefile{toc}{\contentsline {subsubsection}{3.7.1. Broad strategy}{43}{section*.25}}
\newlabel{broad-strategy}{{}{43}{3.7.1. Broad strategy}{section*.25}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {37}{\ignorespaces Rotated number five}}{44}{figure.37}}
\@writefile{toc}{\contentsline {subsubsection}{3.7.2. Learning rate}{45}{section*.26}}
\newlabel{learning-rate}{{}{45}{3.7.2. Learning rate}{section*.26}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {38}{\ignorespaces Comparison of learning rates}}{45}{figure.38}}
\@writefile{toc}{\contentsline {subsubsection}{3.7.3. Use early stopping to determine the number of training epochs}{46}{section*.27}}
\newlabel{use-early-stopping-to-determine-the-number-of-training-epochs}{{}{46}{3.7.3. Use early stopping to determine the number of training epochs}{section*.27}{}}
\@writefile{toc}{\contentsline {subsubsection}{3.7.4. Learning rate schedule}{46}{section*.28}}
\newlabel{learning-rate-schedule}{{}{46}{3.7.4. Learning rate schedule}{section*.28}{}}
\@writefile{toc}{\contentsline {subsubsection}{3.7.5. The regularization parameter}{46}{section*.29}}
\newlabel{the-regularization-parameter}{{}{46}{3.7.5. The regularization parameter}{section*.29}{}}
\@writefile{toc}{\contentsline {subsubsection}{3.7.6. Mini-batch size}{46}{section*.30}}
\newlabel{mini-batch-size}{{}{46}{3.7.6. Mini-batch size}{section*.30}{}}
\@writefile{toc}{\contentsline {subsection}{3.8. Other techniques}{47}{section*.31}}
\newlabel{other-techniques}{{}{47}{3.8. Other techniques}{section*.31}{}}
\newlabel{hessian-technique}{{}{47}{3.8.1. Hessian technique}{section*.32}{}}
\@writefile{toc}{\contentsline {paragraph}{3.8.1. Hessian technique}{47}{section*.32}}
\newlabel{momentum-based-gradient-descent}{{}{48}{Momentum-based gradient descent}{section*.33}{}}
\@writefile{toc}{\contentsline {paragraph}{Momentum-based gradient descent}{48}{section*.33}}
\@writefile{toc}{\contentsline {subsection}{Other models of artificial neuron}{49}{section*.34}}
\newlabel{other-models-of-artificial-neuron}{{}{49}{Other models of artificial neuron}{section*.34}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {39}{\ignorespaces Tanh function}}{50}{figure.39}}
\@writefile{lof}{\contentsline {figure}{\numberline {40}{\ignorespaces Rectifying function}}{50}{figure.40}}
